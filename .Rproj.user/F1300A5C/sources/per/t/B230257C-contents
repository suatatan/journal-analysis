---
title: "Emerald Insights Journal Analysis (2) Year"
author: "Suat ATAN"
date: "27 Şubat 2019"
output: 
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(readr)
emerald_insight <- read_delim("../data/product/emerald_insight.csv", 
    ",", escape_double = FALSE, trim_ws = TRUE)
```

```{r}
library(tidytext)
library(dplyr)
library(stringr)
library(SnowballC)


library(readr)
custom_stop_words <- read_delim("../data/product/cikarilacak_kelimeler.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)

custom_stop_words <- custom_stop_words %>% mutate (word = wordStem(word, language="english"))

books <- emerald_insight %>% select(`article-href`,abstract,issue_date)

tidy_books <- books %>%
  unnest_tokens(word, abstract) %>%
  mutate (word = wordStem(word, language="english")) %>%
  anti_join(stop_words) %>%
  anti_join(custom_stop_words)


# <- tidy_books 
tidy_books <- tidy_books %>% mutate(tarih = substr(issue_date,12,15))
```


# Show Years

```{r}
tidy_books %>% group_by(tarih) %>% count()
```

```{r}
plot(tidy_books %>% group_by(tarih) %>% count(),type="l")
```


# Most Frequent
```{r}
word_freq <- tidy_books %>%
  count(word, sort = TRUE) 
word_freq
```


```{r}
library(ggplot2)

tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 70) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```
# Export

```{r eval=FALSE, include=FALSE}
#Export
export_most_freq<-
tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 2) 
export_most_freq
write.csv(export_most_freq,"../output/most_freq.csv")
# Export N-grams
write.csv(bigram_counts,"../output/ngrams.csv")

write.csv(word_pairs,"../output/word_pairs.csv")

```



# N-Grams

```{r}
austen_bigrams <- books %>%
  unnest_tokens(bigram, abstract, token = "ngrams", n = 2)

library(tidyr)

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% custom_stop_words$word) %>%
  filter(!word2 %in% custom_stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

# Co-Occurence

```{r}
austen_section_words <- books %>%
  unnest_tokens(word, abstract) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!word %in% custom_stop_words$word)

austen_section_words <- austen_section_words %>% mutate(section=`article-href`)
austen_section_words

austen_section_words<-austen_section_words %>% mutate (word = wordStem(word, language="english"))

```

```{r}
library(widyr)

# count words co-occuring within sections
word_pairs <- austen_section_words %>%
  pairwise_count(word, section, sort = TRUE) %>%
  filter(!item1 %in% stop_words$word) %>%
  filter(!item2 %in% stop_words$word) %>%
  filter(!item1 %in% custom_stop_words$word) %>%
  filter(!item2 %in% custom_stop_words$word)

word_pairs
```

# Pairwise Correlation (Let's Look it later)

```{r}
word_cors <- austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 2) %>%
  pairwise_cor(word, section, sort = TRUE)
  #filter(correlation <1)

word_cors
```


# FIXME: Takes time

```{r eval=FALSE, include=FALSE}
# word_cors %>%
# 
#   group_by(item1) %>%
#   top_n(10) %>%
#   ungroup() %>%
#   mutate(item2 = reorder(item2, correlation)) %>%
#   ggplot(aes(item2, correlation)) +
#   geom_bar(stat = "identity") +
#   facet_wrap(~ item1, scales = "free") +
#   coord_flip()
```

```{r include=FALSE}
# library(igraph)
# library(ggraph)
# set.seed(2016)
# 
# word_cors %>%
#   filter(correlation > .9999, correlation <1) %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "fr") +
#   geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name), repel = TRUE) +
#   theme_void()
```

# Term Frequency

First column is article, second is given word in the article third is word in this article

```{r}
book_words <- books %>%
  unnest_tokens(word, abstract) %>%
  mutate (word = wordStem(word, language="english")) %>%
  anti_join(stop_words) %>%
  anti_join(custom_stop_words) %>%
  count(`article-href`, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(`article-href`) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words
```

# Zip's Law

```{r}
freq_by_rank <- book_words %>% 
  group_by(`article-href`) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank
```

# Casting tidy text data into matrix

```{r}
dtm<-freq_by_rank %>%
  cast_dtm(`article-href`, word, n)
#dtm
```

```{r}
library(Matrix)
dtm_mx <- as.matrix(freq_by_rank %>%
  cast_sparse(`article-href`, word, n))
#dtm_mx
```

```{r}
dtm_df <- as.data.frame(dtm_mx)
#head(dtm_df)
```




# Sankey
```{r}
library(alluvial)

keywords <- c('futur','technologi','foresight','econom')
  draw_sankey_diagram <- function(keywords){
  tq <- (word_pairs)
  tq <- word_pairs %>% filter(item1 %in% keywords)
  tq <- head(tq,20)
  alluvial(tq[,1:2],freq=tq$n,
           xw=0.2, alpha=0.2,
           cex=0.7,
           gap.width=0.1, col= "steelblue", border="white"
           )
}
draw_sankey_diagram(keywords)


tq <- (word_pairs)
  tq <- word_pairs %>% filter(item1 %in% keywords)
  tq <- head(tq,20)
 x= alluvial(tq[,1:2],freq=tq$n,
           xw=0.2, alpha=0.2,
           cex=0.7,
           gap.width=0.1, col= "steelblue", border="white"
           )
  x
```


# Sankey 2
```{r}
keywords <- c('culture','nation','univers','econom','univers')
draw_sankey_diagram(keywords)
```


# Sankey 3 (İki alana iki bedava :)

```{r}
draw_sankey_diagram(c('futur','technologi','develop','scenario',''))
```

# Zaman Bazlı Analiz


```{r}

```



