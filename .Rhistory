library(widyr)
# count words co-occuring within sections
word_pairs <- austen_section_words %>%
pairwise_count(word, section, sort = TRUE) %>%
filter(!item1 %in% stop_words$word) %>%
filter(!item2 %in% stop_words$word) %>%
filter(!item1 %in% custom_stop_words$word) %>%
filter(!item2 %in% custom_stop_words$word)
word_pairs
word_cors <- austen_section_words %>%
group_by(word) %>%
filter(correlation() <= 1) %>%
pairwise_cor(word, section, sort = TRUE)
word_cors <- austen_section_words %>%
group_by(word) %>%
filter(n() >= 2) %>%
pairwise_cor(word, section, sort = TRUE)
word_cors
word_cors <- austen_section_words %>%
group_by(word) %>%
filter(n() >= 2) %>%
pairwise_cor(word, section, sort = TRUE) %>%
filter(correlation <1)
word_cors
word_cors <- austen_section_words %>%
group_by(word) %>%
filter(n() >= 2) %>%
pairwise_cor(word, section, sort = TRUE) %>%
#filter(correlation <1)
word_cors
word_cors <- austen_section_words %>%
group_by(word) %>%
filter(n() >= 2) %>%
pairwise_cor(word, section, sort = TRUE)
#filter(correlation <1)
word_cors
#Export
export_most_freq<-
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 2)
export_most_freq
write.csv(export_most_freq,"most_freq.csv")
# Export N-grams
write.csv(bigram_counts,"ngrams.csv")
write.csv(word_pairs,"word_pairs.csv")
knitr::opts_chunk$set(echo = TRUE)
#Export
export_most_freq<-
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 2)
export_most_freq
write.csv(export_most_freq,"../output/most_freq.csv")
# Export N-grams
write.csv(bigram_counts,"../output/ngrams.csv")
write.csv(word_pairs,"../output/word_pairs.csv")
load("C:/Users/suat.atan/Dropbox/NOW/05-Yazilim/journal-analysis/.RData")
knitr::opts_chunk$set(echo = TRUE)
word_freq <- tidy_books %>%
count(word, sort = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
emerald_insight <- read_delim("../data/product/emerald_insight.csv",
",", escape_double = FALSE, trim_ws = TRUE)
library(tidytext)
library(dplyr)
library(stringr)
library(SnowballC)
library(readr)
custom_stop_words <- read_delim("../data/product/cikarilacak_kelimeler.csv",
";", escape_double = FALSE, trim_ws = TRUE)
custom_stop_words <- custom_stop_words %>% mutate (word = wordStem(word, language="english"))
books <- emerald_insight %>% select(`article-href`,abstract)
tidy_books <- books %>%
unnest_tokens(word, abstract) %>%
mutate (word = wordStem(word, language="english")) %>%
anti_join(stop_words) %>%
anti_join(custom_stop_words)
# <- tidy_books
tidy_books
word_freq <- tidy_books %>%
count(word, sort = TRUE)
word_freq
library(ggplot2)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 70) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
austen_bigrams <- books %>%
unnest_tokens(bigram, abstract, token = "ngrams", n = 2)
library(tidyr)
bigrams_separated <- austen_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% custom_stop_words$word) %>%
filter(!word2 %in% custom_stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigram_counts
austen_section_words <- books %>%
unnest_tokens(word, abstract) %>%
filter(!word %in% stop_words$word) %>%
filter(!word %in% custom_stop_words$word)
austen_section_words <- austen_section_words %>% mutate(section=`article-href`)
austen_section_words
austen_section_words<-austen_section_words %>% mutate (word = wordStem(word, language="english"))
library(widyr)
# count words co-occuring within sections
word_pairs <- austen_section_words %>%
pairwise_count(word, section, sort = TRUE) %>%
filter(!item1 %in% stop_words$word) %>%
filter(!item2 %in% stop_words$word) %>%
filter(!item1 %in% custom_stop_words$word) %>%
filter(!item2 %in% custom_stop_words$word)
word_pairs
word_cors <- austen_section_words %>%
group_by(word) %>%
filter(n() >= 2) %>%
pairwise_cor(word, section, sort = TRUE)
#filter(correlation <1)
word_cors
# library(igraph)
# library(ggraph)
# set.seed(2016)
#
# word_cors %>%
#   filter(correlation > .9999, correlation <1) %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "fr") +
#   geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name), repel = TRUE) +
#   theme_void()
book_words <- books %>%
unnest_tokens(word, abstract) %>%
mutate (word = wordStem(word, language="english")) %>%
anti_join(stop_words) %>%
anti_join(custom_stop_words) %>%
count(`article-href`, word, sort = TRUE) %>%
ungroup()
total_words <- book_words %>%
group_by(`article-href`) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
book_words
freq_by_rank <- book_words %>%
group_by(`article-href`) %>%
mutate(rank = row_number(),
`term frequency` = n/total)
freq_by_rank
dtm<-freq_by_rank %>%
cast_dtm(`article-href`, word, n)
#dtm
library(Matrix)
dtm_mx <- as.matrix(freq_by_rank %>%
cast_sparse(`article-href`, word, n))
#dtm_mx
dtm_df <- as.data.frame(dtm_mx)
#head(dtm_df)
library(alluvial)
keywords <- c('futur','technologi','foresight','econom')
draw_sankey_diagram <- function(keywords){
tq <- (word_pairs)
tq <- word_pairs %>% filter(item1 %in% keywords)
tq <- head(tq,20)
alluvial(tq[,1:2],freq=tq$n,
xw=0.2, alpha=0.2,
cex=0.7,
gap.width=0.1, col= "steelblue", border="white"
)
}
draw_sankey_diagram(keywords)
keywords <- c('culture','nation','univers','econom','univers')
draw_sankey_diagram(keywords)
draw_sankey_diagram(c('futur','technologi','develop','scenario',''))
draw_sankey_diagram('2001','2002','2003')
draw_sankey_diagram(c('2001','2002','2003'))
draw_sankey_diagram(c('futur','technologi','develop','scenario',''))
draw_sankey_diagram(c('2000','2001','2002','2003',''))
emerald_insight
library(tidytext)
library(dplyr)
library(stringr)
library(SnowballC)
library(readr)
custom_stop_words <- read_delim("../data/product/cikarilacak_kelimeler.csv",
";", escape_double = FALSE, trim_ws = TRUE)
custom_stop_words <- custom_stop_words %>% mutate (word = wordStem(word, language="english"))
books <- emerald_insight %>% select(`article-href`,abstract,issue_date)
tidy_books <- books %>%
unnest_tokens(word, abstract) %>%
mutate (word = wordStem(word, language="english")) %>%
anti_join(stop_words) %>%
anti_join(custom_stop_words)
# <- tidy_books
tidy_books
x="Published: 1999, Start page: 117"
substr(x,11,17)
substr(x,12,16)
substr(x,12,15)
tidy_books
# <- tidy_books
tidy_books %>% mutate(tarih = substr(issue_date,12,15))
# <- tidy_books
tidy_books
# <- tidy_books
tidy_books %>% mutate(tarih = substr(issue_date,12,15))
# <- tidy_books
tidy_books %>% mutate(tarih = substr(issue_date,12,15))
# <- tidy_books
tidy_books %>% mutate(tarih = substr(issue_date,12,15)) %>% filter (tarih == 2000)
# <- tidy_books
tidy_books %>% mutate(tarih = substr(issue_date,12,15)) %>% filter (tarih == 2000) %>% select(issue_date)
# <- tidy_books
tidy_books %>% mutate(tarih = substr(issue_date,12,15)) %>% filter (tarih == 2000) %>% select(tarih)
# <- tidy_books
tidy_books <- tidy_books %>% mutate(tarih = substr(issue_date,12,15))
tidy_books %>% group_by(tarih) %>% count()
plot(tidy_books %>% group_by(tarih) %>% count())
plot(tidy_books %>% group_by(tarih) %>% count(),type="l")
word_freq <- tidy_books %>%
count(word, sort = TRUE)
word_freq
shiny::runApp('JournalAnalytics')
install.packages(c("yaml", "httpuv"))
install.packages(c("yaml", "httpuv"))
shiny::runApp('JournalAnalytics')
.libPaths()
installed.packages()["httpuv", ]
shiny::runApp('JournalAnalytics')
shiny::runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
knitr::opts_chunk$set(echo = TRUE)
library(readr)
emerald_insight <- read_delim("../data/product/emerald_insight.csv",
",", escape_double = FALSE, trim_ws = TRUE)
library(tidytext)
library(dplyr)
library(stringr)
library(SnowballC)
library(readr)
custom_stop_words <- read_delim("../data/product/cikarilacak_kelimeler.csv",
";", escape_double = FALSE, trim_ws = TRUE)
custom_stop_words <- custom_stop_words %>% mutate (word = wordStem(word, language="english"))
books <- emerald_insight %>% select(`article-href`,abstract,issue_date)
tidy_books <- books %>%
unnest_tokens(word, abstract) %>%
mutate (word = wordStem(word, language="english")) %>%
anti_join(stop_words) %>%
anti_join(custom_stop_words)
# <- tidy_books
tidy_books <- tidy_books %>% mutate(tarih = substr(issue_date,12,15))
tidy_books %>% group_by(tarih) %>% count()
word_freq <- tidy_books %>%
count(word, sort = TRUE)
word_freq
shiny::runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
install.packages(DT)
install.packages("DT")
runApp('JournalAnalytics')
emerald_insight <- read_delim(input$dergi,
",", escape_double = FALSE,
trim_ws = TRUE)
custom_stop_words <- read_delim("cikarilacak_kelimeler.csv",
";", escape_double = FALSE, trim_ws = TRUE)
custom_stop_words <- custom_stop_words %>%
mutate (word = wordStem(word, language="english"))
books <- emerald_insight %>%
select(`article-href`,abstract,issue_date)
tidy_books <- books %>%
unnest_tokens(word, abstract) %>%
mutate (word = wordStem(word, language="english")) %>%
anti_join(stop_words) %>%
anti_join(custom_stop_words)
emerald_insight
journal <- read_delim(input$dergi,
",", escape_double = FALSE,
trim_ws = TRUE)
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
install.packages("Network3D")
runApp('JournalAnalytics')
knitr::opts_chunk$set(echo = TRUE)
library(alluvial)
keywords <- c('futur','technologi','foresight','econom')
draw_sankey_diagram <- function(keywords){
tq <- (word_pairs)
tq <- word_pairs %>% filter(item1 %in% keywords)
tq <- head(tq,20)
alluvial(tq[,1:2],freq=tq$n,
xw=0.2, alpha=0.2,
cex=0.7,
gap.width=0.1, col= "steelblue", border="white"
)
}
draw_sankey_diagram(keywords)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
emerald_insight <- read_delim("../data/product/emerald_insight.csv",
",", escape_double = FALSE, trim_ws = TRUE)
library(tidytext)
library(dplyr)
library(stringr)
library(SnowballC)
library(readr)
custom_stop_words <- read_delim("../data/product/cikarilacak_kelimeler.csv",
";", escape_double = FALSE, trim_ws = TRUE)
custom_stop_words <- custom_stop_words %>% mutate (word = wordStem(word, language="english"))
books <- emerald_insight %>% select(`article-href`,abstract,issue_date)
tidy_books <- books %>%
unnest_tokens(word, abstract) %>%
mutate (word = wordStem(word, language="english")) %>%
anti_join(stop_words) %>%
anti_join(custom_stop_words)
# <- tidy_books
tidy_books <- tidy_books %>% mutate(tarih = substr(issue_date,12,15))
tidy_books %>% group_by(tarih) %>% count()
plot(tidy_books %>% group_by(tarih) %>% count(),type="l")
word_freq <- tidy_books %>%
count(word, sort = TRUE)
word_freq
library(ggplot2)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 70) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
austen_bigrams <- books %>%
unnest_tokens(bigram, abstract, token = "ngrams", n = 2)
library(tidyr)
bigrams_separated <- austen_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% custom_stop_words$word) %>%
filter(!word2 %in% custom_stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigram_counts
austen_section_words <- books %>%
unnest_tokens(word, abstract) %>%
filter(!word %in% stop_words$word) %>%
filter(!word %in% custom_stop_words$word)
austen_section_words <- austen_section_words %>% mutate(section=`article-href`)
austen_section_words
austen_section_words<-austen_section_words %>% mutate (word = wordStem(word, language="english"))
library(widyr)
# count words co-occuring within sections
word_pairs <- austen_section_words %>%
pairwise_count(word, section, sort = TRUE) %>%
filter(!item1 %in% stop_words$word) %>%
filter(!item2 %in% stop_words$word) %>%
filter(!item1 %in% custom_stop_words$word) %>%
filter(!item2 %in% custom_stop_words$word)
word_pairs
word_cors <- austen_section_words %>%
group_by(word) %>%
filter(n() >= 2) %>%
pairwise_cor(word, section, sort = TRUE)
#filter(correlation <1)
word_cors
# library(igraph)
# library(ggraph)
# set.seed(2016)
#
# word_cors %>%
#   filter(correlation > .9999, correlation <1) %>%
#   graph_from_data_frame() %>%
#   ggraph(layout = "fr") +
#   geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
#   geom_node_point(color = "lightblue", size = 5) +
#   geom_node_text(aes(label = name), repel = TRUE) +
#   theme_void()
book_words <- books %>%
unnest_tokens(word, abstract) %>%
mutate (word = wordStem(word, language="english")) %>%
anti_join(stop_words) %>%
anti_join(custom_stop_words) %>%
count(`article-href`, word, sort = TRUE) %>%
ungroup()
total_words <- book_words %>%
group_by(`article-href`) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
book_words
freq_by_rank <- book_words %>%
group_by(`article-href`) %>%
mutate(rank = row_number(),
`term frequency` = n/total)
freq_by_rank
dtm<-freq_by_rank %>%
cast_dtm(`article-href`, word, n)
#dtm
library(Matrix)
dtm_mx <- as.matrix(freq_by_rank %>%
cast_sparse(`article-href`, word, n))
#dtm_mx
dtm_df <- as.data.frame(dtm_mx)
#head(dtm_df)
library(alluvial)
keywords <- c('futur','technologi','foresight','econom')
draw_sankey_diagram <- function(keywords){
tq <- (word_pairs)
tq <- word_pairs %>% filter(item1 %in% keywords)
tq <- head(tq,20)
alluvial(tq[,1:2],freq=tq$n,
xw=0.2, alpha=0.2,
cex=0.7,
gap.width=0.1, col= "steelblue", border="white"
)
}
draw_sankey_diagram(keywords)
tq <- (word_pairs)
tq <- word_pairs %>% filter(item1 %in% keywords)
tq <- head(tq,20)
alluvial(tq[,1:2],freq=tq$n,
xw=0.2, alpha=0.2,
cex=0.7,
gap.width=0.1, col= "steelblue", border="white"
)
tq <- (word_pairs)
tq <- word_pairs %>% filter(item1 %in% keywords)
tq <- head(tq,20)
x= alluvial(tq[,1:2],freq=tq$n,
xw=0.2, alpha=0.2,
cex=0.7,
gap.width=0.1, col= "steelblue", border="white"
)
x
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
a="ali,"veli""
a="ali,veli"
a.split(",")
a
strsplit(a,",")
strsplit(a,",")[0]
strsplit(a,",")[[0]]
as.vector(strsplit(a,","))
as.vector(strsplit(a,",")[[0]])
unlist(strsplit("a b c", split=" "))
unlist(strsplit("a b c", split=" "))
x=c("a","b","c")
x
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
3+4
price <- 10
ratio <-20
print(price*ratio)
shiny::runApp('JournalAnalytics')
shiny::runApp('JournalAnalytics')
install.packages("shiny")
shiny::runApp('JournalAnalytics')
shiny::runApp('JournalAnalytics')
runApp('JournalAnalytics')
runApp('JournalAnalytics')
a="Volume 29, Issue 3, July 2007, Pages 357-374"
b="Published: 1999, Start page: 117"
library("stringr")
str_extract(a,"\d")
str_extract(a,"\\d")
str_extract(a,"^\d{4}$")
str_extract(a,"^\\d{4}$")
str_extract_all(a,"^\d{4}$")
str_extract_all(a,"^\\d{4}$")
str_extract_all(a,"(19|20)\d{2}$")
str_extract_all(a,"(19|20)\\d{2}$")
str_extract(a,"(19|20)\\d{2}$")
str_extract(a,"^(19|20)\\d{2}$")
str_extract(a,"(19|20)\\d{2}")
str_extract(b,"(19|20)\\d{2}")
runApp('JournalAnalytics')
